1. If Loss doesn't converge, try changing number of layers and layer sizes.

Observation - Training loss for every epoch was same.
Variations tried to improve the problem - changing normality of initialization, changing loss function
What worked - Changing the network architecture.
References - https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw
There's one additional rule of thumb that helps for supervised learning problems. The upper bound on the number of hidden neurons that won't result in over-fitting is:

Nh=Ns(α∗(Ni+No))

Ni = number of input neurons.
No = number of output neurons.
Ns = number of samples in training data set.
α = an arbitrary scaling factor usually 2-10.

2. For Overfitting

2 ways to identify overfitting - 

a. validation loss > training loss (for later no. of epochs)
b. Accuracy remains constant inspite of drop in training loss.

To prevent overfitting - 

a. Stop training around the epoch where validation loss > training loss
